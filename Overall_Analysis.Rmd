---
title: "Simulation Analysis"
output: html_document
---
This document summarizes the analysis supporting our findings in the main paper text. It includes both the code, results, and our interpretation of the results. The contents of this document can be modified and run.

## Set up

This section effectively setsup the analysis environment. It includes the different R packages used in the data, the data, and finally additional custom functions that have been written to support analysis.

**Packages and Data**
```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(philentropy)
library(tidyr)

#load data
runs<-read.csv(file="sample_data/runs_sample.csv",stringsAsFactors = FALSE)
topic_term<-read.csv(file="sample_data/pred_topics_terms_sample.csv",stringsAsFactors = FALSE)
topic_doc<-read.csv(file="sample_data/pred_topics_docs_sample.csv",stringsAsFactors = FALSE)
gt_class<-read.csv(file="sample_data/gt_topics_sample.csv",stringsAsFactors = FALSE)
docs<-read.csv(file="sample_data/documents_sample.csv",stringsAsFactors = FALSE)

```


**Customized functions**

Custom functions for quickly computing cosine and jensen shannon distances from a matrix of probabilities.
```{r}
#generic function of handle calculation of jensen_shannon distance from matrix
JS<-function(prob_matrix){
    tmp<-apply(prob_matrix,1,function(x){
      apply(prob_matrix,1,function(x,y){
        jensen_shannon(x,y,testNA = FALSE, unit = "log2")
      },x = x)
    })
    
    return(tmp)
}

#generic function of handle calculation of cosine distance from matrix
CS<-function(prob_matrix){
    tmp<-apply(prob_matrix,1,function(x){
      apply(prob_matrix,1,function(x,y){
        cosine_dist(x,y,testNA = FALSE)
      },x = x)
    })
    
    return(tmp)
}

```


## Dataset overview

We use the Reuters-21578 benchmark dataset to inspect the effects of user interactions with text analytics pipelines. The dataset has 10,788 documents that have been manually assigned to one or more topics of a possible set of 90 topics. We limit our analysis to documents that have only one topicassigned to them, which is 9,160 (84%) of all documents. These 9,160 documents are represent a subset of 65 of the initial 90 topics.

The same set of documents and ground truth topics are used across multiple runs of the data. We use the data from a single run to visualizae the characteristics of the this dataset.
```{r}
#get data from a single run
run_ids<-runs %>% select(run_id) %>% unique()
tmp_gt<-gt_class %>% filter(run_id == run_ids$run_id[1])

```


**Size of ground truth datasets**

There are a total of 65 'ground truth' topics in this dataset. They vary in size from `min(tmp_gt$grp_size)` to `max(tmp_gt$grp_size)`. There is a long tail, in the the majority of the documents belong to two topics.

```{r}
tmp_gt %>% 
  ggplot(aes(x=reorder(category,-grp_size), y = grp_size)) +
  geom_bar(stat="identity")+
  theme_bw()+
  labs(y="Total Number of Documents",x="Topic")+
  theme(axis.text.x = element_text(angle = 90, hjust=1,vjust=0.5))
```



## Evaluations of User Interactions - Run Analysis

A run here captures the changes to the output for our text analytics pipelines based upon one single user action.  We see the results according to three types of metrics:

1) Benchmark metrics
2) Cluster metrics
3) Topic metrics.


## Benchmark Metrics

```{r}
runs %>%
  dplyr::select(run_id,action_impact,action_type) %>%
  distinct()
```

We can see how metrics change over individual runs
```{r,fig.width=10,unit="in"}
runs %>%
  ggplot(aes(x=run_id,y=metric_score)) +
  geom_bar(stat="identity",aes(fill = action_impact),color="grey")+
  facet_grid(.~metric)+
  theme_bw()+
  scale_fill_brewer(palette = "Greys",direction=-1)+
  theme(axis.text.x = element_text(angle=90,hjust=1,vjust=0.5))
```

or all runs together
```{r,fig.width=10,unit="in"}
runs %>%
  ggplot(aes(x=metric,y=metric_score)) +
  geom_boxplot(outlier.alpha=0)+
  geom_jitter(shape=21,size=4,aes(fill=action_impact),width=0.2,height=0)+
  facet_grid(.~metric,scales = "free_x")+
  scale_y_continuous(limits=c(0,1))+
  theme_bw()+
  scale_fill_brewer(palette = "Greys",direction=-1)+
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

We can also see the differences of metrics within an individual run:

```{r,fig.width=10,unit="in"}
runs %>%
  ggplot(aes(x=metric,y=metric_score)) +
  geom_bar(stat="identity",aes(fill = action_impact),color="grey")+
  facet_grid(.~run_id)+
  theme_bw()+
  scale_fill_brewer(palette = "Greys",direction=-1)+
  theme(axis.text.x = element_text(angle=90,hjust=1,vjust=0.5))
```

What we can conclude is that individual metrics would measures the quality of the change differently. Accuracy is the most sentistive to changes, but f1 and fm1, which measure both repcision and accuracy is not. Metrics that look at the quality of the clustering, or in other terms, how similar documents to other documents assigned to the same topic. Compared to accuracy those metrics are far more stable and don't change very much even when the user makes from substantive changes.

Why is that?

The dataset that we are using has two very large classes and this seems to dominate both the algorithm's ability to select topics and the to assess their quality. This observations underscores the important relationship between the underlying structure of the data and the machine learning model. To some extent, hyper paramter tuning achieves very little.

### Topic Metrics
Many individuals are also interest in the quality of the topics, usually examined by looking closely at the distribution of terms within topics. We examine those here.

First off, its easy to see that looking across the top terms from all runs quickly produces a visualization that is to visbly dense to parse. It might be worth comparing runs that have different extremes.

```{r}
topic_term %>%
  filter(top_term_order<=10) %>% #select top ten terms only
  ggplot(aes(y=top_term,x=run_id)) +
  geom_tile() +
  theme_bw()+
  theme(axis.text.x = element_text(angle=90,hjust=1,vjust=0),axis.text.y = element_blank())
  
```

### Per run term similarity

Here we compute the KL divergence within topics of a common run and also between runs.

```{r}
run_topics<-dplyr::filter(topic_term, run_id == '2912314e-a515-423') %>%
  dplyr::select(topic_id,top_term,top_prob) %>%
  tidyr::pivot_wider(names_from = top_term, values_from = top_prob) #values fill isn't work.. hmm

run_topics[is.na(run_topics)]<-0 #shouldn't have to do this
prob_matrix<-as.matrix(run_topics[,2:ncol(run_topics)])
```

#### KL Divergence
```{r}
KLMatrix<-KL(prob_matrix)
heatmap(KLMatrix)
```
```{r}
#gotta do this manually
JS_matrix<-JS(prob_matrix)
heatmap(JS_matrix)
```

## Topic Document Analysis
```{r}
CS_matrix<-CS(prob_matrix)
heatmap(CS_matrix)
```

```{r}

```
## Document Analysis